{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega final equipo Astralaria\n",
    "### El equipo formado por Asier Serrano Aramburu y Mario Campos Mocholí de la Universitat Politécnica de València, entrega este archivo como el generador de la predicción final del reto Minsait Land Classification, datathon University Hack 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice.\n",
    "### 1. Imports.\n",
    "### 2. Obtención de los datasets.\n",
    "### 3. Visualizaciones.\n",
    "### 4. Feature Engineering.\n",
    "### 5. Modelo final XGBClassifier + RandomForest\n",
    "### 6. Generación del fichero resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías externas.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import random\n",
    "\n",
    "#Métodos realizados por Astralaria, en Astralarialib.\n",
    "from Astralarialib.datasets_get import get_modelar_data, getX, getY, get_mod_data_original, get_estimar_data, get_modelar_data_ids, get_categorical_decoder_class\n",
    "from Astralarialib.feature_engineering import coordinates_fe\n",
    "from Astralarialib.random_undersampling import random_undersample_residential\n",
    "from Astralarialib.visualization import pca_general, tsne, hist_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtención de los datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sin ID, para las visualizaciones.\n",
    "#Dataset modelar sin IDs, sin preprocesamiento.\n",
    "mod_or = get_mod_data_original()\n",
    "#Dataset modelar original sin IDs y sin la variable clase.\n",
    "X_mod_or = getX(mod_or)\n",
    "#Dataset modelar original sin IDs y solo con la variable clase.\n",
    "Y_mod_or = getY(mod_or)\n",
    "\n",
    "#Con ID, para el FE.\n",
    "#Dataset estimar, preprocesado.\n",
    "X_estimar = get_estimar_data()\n",
    "#Dataset modelar sin la variable clase.\n",
    "X_modelar = getX(modelar_df)\n",
    "#Dataset modelar, solo la variable clase.\n",
    "Y_modelar = getY(modelar_df)\n",
    "#Dataset modelar con IDs, preprocesado y con Random Undersampling de RESIDENTIAL.\n",
    "modelar_df = random_undersample_residential(get_modelar_data_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_categories_list' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-202bb35228e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Histograma descompensación.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#PCA 2D y PCA 3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpca_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_modelar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_modelar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#t-SNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Projects/Python/UniversityHack-2020/Entorno de la fase final/Astralarialib/visualization.py\u001b[0m in \u001b[0;36mhist_decomposition\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcategories_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_categories_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodelar_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_modelar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcount_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategories_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcount_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategories_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelar_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelar_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CLASS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_categories_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Histograma descompensación.\n",
    "hist_decomposition()\n",
    "#PCA 2D y PCA 3D\n",
    "pca_general(X_modelar, Y_modelar)\n",
    "#t-SNE\n",
    "tsne(X_modelar, Y_modelar, [50], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos los nuevos datasets con las 7 features de los K-1 vecinos más próximos\n",
    "X_modelar, X_estimar, est_IDS = coordinates_fe(X_modelar, Y_modelar, X_estimar)\n",
    "#Ajustamos para numpy arrays.\n",
    "Y_modelar = Y_modelar.values\n",
    "\n",
    "Y_modelar = Y_modelar[1:, :]\n",
    "X_estimar = X_estimar[1:, :]\n",
    "X_modelar = X_modelar[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelo final XGBClassifier + RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable que contendrá las muestras separadas por clase\n",
    "data_per_class = []\n",
    "\n",
    "# Añadimos una lista vacía por clase\n",
    "for _ in range(7):         \n",
    "    data_per_class.append([])\n",
    "\n",
    "# Añadimos a la lista de cada clase las muestras de esta\n",
    "for i in range(len(X_modelar)):\n",
    "    data_per_class[int(Y_modelar[i])].append(X_modelar[i, 1:].tolist() + Y_modelar[i].tolist())\n",
    "\n",
    "\n",
    "# Variable que contendrá los datos procesados\n",
    "data_proc = []\n",
    "\n",
    "\n",
    "# Variable que contendra las predicciones globales de cada muestra\n",
    "predictions = {}\n",
    "\n",
    "# Número de iteraciones total por módelo\n",
    "iterations = 2\n",
    "\n",
    "# Si True, muestra información de cada modelo local tras entrenarlo\n",
    "debug_mode = True\n",
    "\n",
    "# Si True, guarda los mejores modelos para poder replicar el clasificador\n",
    "persistent_mode = False\n",
    "\n",
    "# Variable en el rango (0.0 - 1.0) que indica el procentaje de muestras de validación\n",
    "test_avg = 0.20\n",
    "\n",
    "# Variable en el rango (0.0 - 1.0) que indica el procentaje de mejores modelos a utilizar\n",
    "best_model_avg = 0.5\n",
    "\n",
    "# Variables que miden las métricas globlales del ENTRENAMIENTO\n",
    "accuracy_avg = 0\n",
    "precision_avg = 0\n",
    "recall_avg = 0\n",
    "f1_avg = 0\n",
    "\n",
    "# Lista que contendra diccionarios con las metricas de cada modelo, predicciones y conjunto de datos utilizados\n",
    "concensus = []\n",
    "\n",
    "# Los diccionarios anteriores seguiran el siguiente formato:\n",
    "'''\n",
    "model_info = {\n",
    "    \"accuracy\":\n",
    "    \"precision\":\n",
    "    \"recall\":\n",
    "    \"f1\":\n",
    "    \"predictions\":\n",
    "    \"data\": {\n",
    "            'X_train': X_train, \n",
    "            'X_test': X_test, \n",
    "            'y_train': y_train, \n",
    "            'y_test': y_test,\n",
    "        },\n",
    "    \"model\": <- Solo si 'persistent_mode' es True\n",
    "}\n",
    "'''\n",
    "\n",
    "for ite in range(iterations):\n",
    "    data_proc = []\n",
    "    # Muestras de la clase RESIDENTIAL\n",
    "    random.shuffle(data_per_class[0])\n",
    "    data_proc += data_per_class[0][:6000]\n",
    "\n",
    "    # Muestras de las otras clases\n",
    "    for i in range(6):\n",
    "        data_proc += data_per_class[i + 1]\n",
    "        \n",
    "    # Volvemos a convertir los datos una vez procesados a una matriz\n",
    "    data_proc = np.array(data_proc)\n",
    "\n",
    "    # Obtenemos una separación del conjunto de train y test equilibrado (respecto a porcentaje de cada clase)\n",
    "    pos = len(data_proc[0]) - 1\n",
    "    X, Y = (data_proc[:, :pos], data_proc[:, pos])\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = test_avg)\n",
    "\n",
    "    for train_index, test_index in sss.split(X, Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "\n",
    "    # Mostramos el porcentaje de entrenamiento\n",
    "    print('Entrenamiento completo al {}%'.format(ite/iterations * 100))\n",
    "    \n",
    "    # Modelo XGB\n",
    "    model = xgb.XGBClassifier(\n",
    "        # General parameters\n",
    "        # Tree Booster parameters\n",
    "        eta = 0.15,\n",
    "        max_depth = 10,\n",
    "        n_estimators = 240,\n",
    "        tree_method = 'exact',\n",
    "        # Learning task parameters\n",
    "        objective = 'multi:softmax',\n",
    "        num_class =  7,\n",
    "        eval_metric = 'merror',\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Métricas del modelo entrenado\n",
    "    if debug_mode:\n",
    "        print('XGBoost ({})'.format(ite))\n",
    "        print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "        print('Precision (macro): {}'.format(precision_score(y_test, y_pred, average = 'macro')))\n",
    "        print('Recall (macro): {}'.format(recall_score(y_test, y_pred, average = 'macro')))\n",
    "        print('F1 (macro): {}\\n'.format(f1_score(y_test, y_pred, average = 'macro')))\n",
    "    \n",
    "    # Actualización de las métricas de ENTRENAMIENTO\n",
    "    accuracy_avg += accuracy_score(y_test, y_pred)\n",
    "    precision_avg += precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall_avg += recall_score(y_test, y_pred, average = 'macro')\n",
    "    f1_avg += f1_score(y_test, y_pred, average = 'macro')\n",
    "\n",
    "    # Diccionario con la información del modelo\n",
    "    model_info = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average = 'macro'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average = 'macro'),\n",
    "        \"f1\": f1_score(y_test, y_pred, average = 'macro'),\n",
    "        \"predictions\": model.predict(X_estimar[:, 1:].astype('float32')),\n",
    "        \"data\": {\n",
    "            'X_train': X_train, \n",
    "            'X_test': X_test, \n",
    "            'y_train': y_train, \n",
    "            'y_test': y_test,\n",
    "        },\n",
    "    }\n",
    "    if persistent_mode:\n",
    "        model_info['model'] = model\n",
    "    concensus.append(model_info)\n",
    "\n",
    "        \n",
    "    # Modelo RandomForest\n",
    "    model = RandomForestClassifier(\n",
    "        criterion = 'entropy',\n",
    "        n_jobs = -1,\n",
    "        max_features = None,\n",
    "        n_estimators = 400,\n",
    "        max_depth = 50,\n",
    "        min_samples_split = 3,\n",
    "        min_samples_leaf = 1,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métricas del modelo entrenado\n",
    "    if debug_mode:\n",
    "        print('RF ({})'.format(ite))\n",
    "        print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "        print('Precision (macro): {}'.format(precision_score(y_test, y_pred, average = 'macro')))\n",
    "        print('Recall (macro): {}'.format(recall_score(y_test, y_pred, average = 'macro')))\n",
    "        print('F1 (macro): {}\\n'.format(f1_score(y_test, y_pred, average = 'macro')))\n",
    "    \n",
    "    # Actualización de las métricas de ENTRENAMIENTO\n",
    "    accuracy_avg += accuracy_score(y_test, y_pred)\n",
    "    precision_avg += precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall_avg += recall_score(y_test, y_pred, average = 'macro')\n",
    "    f1_avg += f1_score(y_test, y_pred, average = 'macro')\n",
    "\n",
    "    # Diccionario con la información del modelo\n",
    "    model_info = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average = 'macro'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average = 'macro'),\n",
    "        \"f1\": f1_score(y_test, y_pred, average = 'macro'),\n",
    "        \"predictions\": model.predict(X_estimar[:, 1:].astype('float32')),\n",
    "        \"data\": {\n",
    "            'X_train': X_train, \n",
    "            'X_test': X_test, \n",
    "            'y_train': y_train, \n",
    "            'y_test': y_test,\n",
    "        },\n",
    "    }\n",
    "    if persistent_mode:\n",
    "        model_info['model'] = model\n",
    "    concensus.append(model_info)\n",
    "\n",
    "\n",
    "print('\\nEntrenamiento completo\\n')\n",
    "print('MÉTRICAS DEL ENTRENAMIENTO (global)')\n",
    "print('Accuracy: {}'.format(accuracy_avg / (iterations * 2)))\n",
    "print('Precision (macro): {}'.format(precision_avg / (iterations * 2)))\n",
    "print('Recall (macro): {}'.format(recall_avg / (iterations * 2)))\n",
    "print('F1(macro): {}'.format(f1_avg / (iterations * 2)))\n",
    "\n",
    "# 1. Ordenamos 'concensous' según una métrica\n",
    "concensus = sorted(concensus, key = lambda i: i['f1'], reverse = True)\n",
    "\n",
    "# 2. Obtenemos los 'x' mejores modelos\n",
    "n = int(iterations * 2 * best_model_avg)\n",
    "\n",
    "# 3. Calculamos la métrica general para los 'x' modelos y predecimos\n",
    "accuracy_avg = 0\n",
    "precision_avg = 0\n",
    "recall_avg = 0\n",
    "f1_avg = 0\n",
    "\n",
    "for i in range(n):\n",
    "    # Métricas\n",
    "    accuracy_avg += concensus[i]['accuracy']\n",
    "    precision_avg += concensus[i]['precision']\n",
    "    recall_avg += concensus[i]['recall']\n",
    "    f1_avg += concensus[i]['f1']\n",
    "\n",
    "    # Predicciones\n",
    "    predictions_aux = model.predict(X_estimar[:, 1:].astype('float32'))\n",
    "    for i in range(len(X_estimar)):\n",
    "        if (X_estimar[i, 0] not in predictions):\n",
    "            predictions[X_estimar[i, 0]] = [int(predictions_aux[i])]\n",
    "        else:\n",
    "            predictions[X_estimar[i, 0]].append(int(predictions_aux[i])) \n",
    "\n",
    "print('\\nMÉTRICAS DEL MODELO (concenso)')\n",
    "print('Accuracy: {}'.format(accuracy_avg / n))\n",
    "print('Precision (macro): {}'.format(precision_avg / n))\n",
    "print('Recall (macro): {}'.format(recall_avg / n))\n",
    "print('F1 (macro): {}'.format(f1_avg / n))\n",
    "\n",
    "# 4. Guardamos la partición que mejor resultado ha dado\n",
    "# TODO: concensus[0]['data'] -> fichero csv\n",
    "np.savetxt(r'Resultados/Res_FE-FINAL_(X_train).csv', concensus[0]['data']['X_train'], delimiter = '|')\n",
    "np.savetxt(r'Resultados/Res_FE-FINAL_(X_test).csv', concensus[0]['data']['X_test'], delimiter = '|')\n",
    "np.savetxt(r'Resultados/Res_FE-FINAL_(y_train).csv', concensus[0]['data']['y_train'], delimiter = '|')\n",
    "np.savetxt(r'Resultados/Res_FE-FINAL_(y_test).csv', concensus[0]['data']['y_test'], delimiter = '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para decodificar el nombre de las clases\n",
    "categorical_decoder_class = get_categorical_decoder_class()\n",
    "\n",
    "def most_frequent(lst): \n",
    "    return max(set(lst), key = lst.count) \n",
    "\n",
    "\n",
    "with open(r'Resultados/Res_FE-FINAL', 'w') as write_file:\n",
    "    write_file.write('ID|CLASE\\n')\n",
    "    for sample in X_estimar:\n",
    "        write_file.write('{}|{}\\n'.format(sample[0], categorical_decoder_class[most_frequent(predictions[sample[0]])]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit45e459bd1fbe41c284fa81861785a8bf",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}